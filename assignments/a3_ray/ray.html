<!-- <h1><strong>PA 2: Ray1</strong></h1> -->
<script type="text/javascript" async
  src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-MML-AM_CHTML">
</script>

<p><strong>Do this project alone or in groups of two, as you prefer. </strong>You can use <a href="https://piazza.com/class/jd0ealxkqom6s8">Piazza</a> to help pair yourselves up.

<!-- <h3>Programming Part</h3> -->

<h4><strong>A. Ray Tracing Part 1</strong></h4>
<p>
<strong>Due: Friday, Oct 7 2019 (11:59pm)</strong> <br>
<!-- <strong>File: </strong> <a class="link" href="materials/mesh.pdf">Mesh(pdf)</a><br> -->
<!-- <p> -->
<!-- Please use the <a href="demos/obj_viewer/viewer.html">visualization tool</a> for verifying your answers. -->
<strong>Points:</strong> 100 <br>
<strong>Submission:</strong> a ZIP file upload <br>
<strong>This assignment comes with multiple subtasks, Please make sure to start early.</strong>

<!-- <h3><strong>Due: Friday September 21st 2018 (11:59pm)</strong></h3> -->

<h3>1.Introduction</h3>

<p>Ray tracing is a simple and powerful algorithm for rendering images. Within the accuracy of the scene and shading models and with enough computing time, the images produced by a ray tracer can be physically accurate and can appear indistinguishable from real images. Cornell pioneered research in accurate rendering. See <a href="http://www.graphics.cornell.edu/online/box/compare.html">http://www.graphics.cornell.edu/online/box/compare.html</a> for the famous Cornell box, which exists in Rhodes Hall.

<p>The basic idea behind ray tracing is to model the movement of photons as they travel along their path from a light source, to a surface, and finally to a viewer's eye. 
  Tracing forward along this path is difficult, since it can be unclear which photons will make it to the eye, especially when photons can bounce between objects several times along their path (and in unpredictable ways). 
  To simplify the system, these paths are computed in reverse; that is, rays originate from the position of the eye. Color information, which depends on the scene's light sources, is computed when these rays intersect an object, and is sent back towards the eye to determine the color of any particular pixel.


<p>We have provided framework code to save you from spending time on class
hierarchy design and input/output code, and rather to have you focus on implementing the core ray
tracing components. However, you also have the freedom to redesign the system
as long as your ray tracer meets our requirements and produces the same images
for the given scenes.

<h4>Requirement Overview</h4>

<p>A ray tracer computes images in <i>image order</i>, meaning that the outer loop visits pixels one at a time.  (In our framework the outer loop is found in the class <tt>RayTracer</tt>).  For each pixel it performs the following operations:
<ul style="list-style: none;">
  <li><strong>Ray generation</strong>, in which the viewing ray corresponding to the current pixel is found.  The calculations depend on the characteristics of the camera: where it is located, which way it is looking, etc.  In our framework, ray generation happens in the subclasses of <tt>Camera</tt>.
  <li><strong> Ray intersection</strong>, which determines the visible surface at the current pixel by finding the closest intersection between the ray and a surface in the scene (in our framework, ray intersection is handled by the <tt>Scene</tt> class).
  <li><strong>Shading</strong>, in which the color of the object, as seen from the camera, is determined.  Shading accounts for the effects of illumination and shadows (in our framework, shading happens in the subclasses of <tt>Shader</tt>).
</ul>

In this assignment, your ray tracer will have support for:
<ul>
  <li>Ray generation: Parallel and perspective cameras</li>
  <li>Ray Intersection: Intersection with spheres and triangle meshes</li>
  <li>Ray Tracing and Shading</li>
    <ul>
        <li>Lambertian and Phong shading</li>
        <li>Mirror reflections</li>
        <li>Point lights with shadows</li>
    </ul>
  <!-- <li>Recursive refectance/refractance (Extra credit)</li> -->
</ul>

<p>Your ray tracer will read files in an XML file format and output images in the OpenEXR high-dynamic-range (HDR) image format. We have split ray tracing in CS4620 into two parts. For now, you will implement basic ray tracing. Later in the next assignment, after you have learned accelerate structures, you will implement features for faster rendering. In this document, we focus on what you need to implement the basic elements of a ray tracer.

<h5>Precisions in the programming assignment</h5>
<p>All the input in this project is parsed in single precision and the scene is stored in single precision as well.  This is a good practice since scene data in real systems can occupy a lot of memory. But accurate ray intersections and shadows are difficult to achieve with single precision.  So you should carry out all the geometric calculations, and store all related intermediate results, in double precision.

<!-- 

<p>A new commit has been pushed to the class Github page in the master branch. We recommend
switching to your master branch, pulling this commit, creating a new branch (e.g. Ray
solution), and committing your work there. This new commit contains all the framework changes and additions necessary for this project. -->



<!-- <h4>Specification and Implementation</h4> -->


<h3>2. Programming Tasks</h3>
<p>To get the basecode, if you already have the course framework repository on your machine, do <code>git pull</code> in the folder "frameworks_cs4620fa19/".
  If you don't have the basecode on your machine, you can download it by git clone:</p>

<pre class="dark">git clone https://github.com/smarschner/frameworks_cs4620fa19</pre>


Once you have the basecode downloaded, you can find the functions to be implemented by searching <tt>TODO#Ray</tt>.
To see all these TODO's in Eclipse, select <tt>Search</tt> menu, then <tt>File Search</tt>
and type <tt>"TODO#Ray"</tt>. There are quite a few separate code snippets to
complete in this assignment. To check your progress as you go along, we
recommend implementation in the following order.

<h4>Getting Started</h4>

<p>
    The ray1 framework is set up to output high-dynamic range (HDR) images in the OpenEXR format.  We use the <tt>openexr</tt> C++ library to produce HDR output, via a JNI library called <tt>openexrjni</tt>. In order for the Java runtime to find the native library that is appropriate to your platform, you need to configure your native library location. To do this in Eclipse, go to Project → Properties → Java Build Path → Select Libraries → Select the openexrjni-3.0.0.jar dropdown menu → Modify Native Library Location. Modify this setting so that it matches your OS. Also in Project → Properties → Java Build Path → Select Source → Select the ray/src. Drop Down Menu → Modify Native Library Location. Modify this setting so that it matches your OS.
  </p>
  <p>
  If you are on Linux, you will additionally need to add <tt>deps/native/linux</tt> to <tt>LD_LIBRARY_PATH</tt>. One way to do so is to launch Eclipse as follows in a terminal, where the actual path to <tt>deps/native/linux</tt> is substituted in for <tt>&lt;path/to/deps/native/linux&gt;</tt>:
  </p>
  <pre>
    $ LD_LIBRARY_PATH=&lt;path/to/deps/native/linux&gt;:$LD_LIBRARY_PATH eclipse
  </pre>
  <p>
    The HDR output file will have a <tt>.exr</tt> extension. HDR images are becoming fairly widely supported; handy tools you can use to view them include, and some of them are particular useful in comparing two images:
    <ul>
    <li>You can use <a href="https://github.com/Tom94/tev">tev</a> image viewer to view EXR images. 
    <li><a href="https://bitbucket.org/wkjarosz/hdrview">HDRView</a>, a minimal but convenient viewer that is cross-platform but comes precompiled only for MacOS.
    <li><a href="http://www.irfanview.com/">IrfanView</a>, a mature and full featured image manipulation program that's free for non-commercial use but is Windows-only.
    <li>The qt4image app, part of <a href="https://bitbucket.org/edgarv/hdritools">HDRITools</a> package from which our JNI library also comes, which is cross-platform but very minimal.
    <li>The Preview app that comes with MacOS, which is handy (you can adjust exposure in the Adjust Color dialog) but has a habit of overwriting your files without asking.
    <li>Photoshop, the widely used image manipulation program, if you already happen to own a license.
    </ul>
  
    The <tt>writeHDR</tt> variable in <tt>RayTracer</tt> can be set to false to output PNG images, but pixel values above 1.0 will be clipped.
  </p>

<h3>Task 1: Generating Rays from Cameras (20 points)</h3>

<p>Here in this task, you are going to generate rays by implementing two subclasses of the <code>Camera</code> class, including orthographic camera and perspective camera. </p>
<p>You can find the functions to implement in  <code>OrthographicCamera.java</code> and <code>PerspectiveCamera.java</code>.

<p><tt>OrthographicCamera</tt> and <tt>PerspectiveCamera</tt> are both <tt>Camera</tt> subclasses representing a camera in the scene. For each of these cameras you need to
   implement the following methods: 

<p> 1. <code>public void init()</code>. This method performs one-time setup after the scene (including the camera) is read and before rendering begins.  
You will want to use it to set up an orthonormal basis for the camera based on the view and up directions, so that you don't waste time doing this computation for every ray. 
You can either store this orthonormal basis as a matrix, or several basis vectors.

<p> 2. <code> public void getRay(Ray outRay, double inU, double inV)</code>. This method generates a new ray starting at the camera through a given image point and stores it in <tt>outRay</tt>. 
  The image point is given in UV coordinates, which must first be converted into view coordinates. 
  Recall that an orthographic camera produces rays with varying origin and constant direction, whereas a perspective camera produces rays with constant origin and varying direction.

<!-- <p>You will implement these two methods for both the <tt>OrthographicCamera</tt> and the <tt>PerspectiveCamera</tt>.  -->
All of the properties you will need are stored in the <tt>Camera</tt> class.

<p>You can then test your getRay() method for both of these classes by running the main method in Camera.java. You should get "Test successful" messages if your implementation is correct.


<h3>Task 2: Intersecting Ray with Surfaces (30 points)</h3>

<p>Here in this task, you are going to implement the function <code>boolean intersect(IntersectionRecord outRecord, Ray rayIn)</code> in both <code>Sphere.java</code> and <code>Triangle.java</code>. 
<p> This method intersects the given ray with the surface. Return true if the ray intersects the geometry 
      and write relevant information to the IntersectionRecord structure. Relevant information 
      includes the position of the hit point <code>.location</code>, the surface normal at the hit point <code>.normal</code>, 
      the value <code>t</code> of the ray at the hit point, the texture coordinates of the hit point <code>.texCoords</code>
      and the surface being hit itself <code>.surface</code>. See the <code>IntersectionRecord</code> class for details.


<!-- <p>An instance of the <tt>Surface</tt> class represents a piece of geometry in the scene. It has to implement the following method: -->
<!-- <pre>
  boolean intersect(IntersectionRecord outRecord, Ray ray)
</pre> -->

<p>We ask that you implement the intersection methods for the following surfaces:
<ul style="list-style: none;">
  <li><strong><tt>1.Sphere</strong></tt>: The class contains a 3D point <tt>center</tt> and a float <tt>radius</tt>. You will need to solve the sphere/ray intersection equations. Be sure to calculate the exact normal at the point of intersection and store it in the <tt>IntersectionRecord</tt>. Additionally, please compute and store UV-coordinates in the same orientation as Assignment 1 (i.e. the poles should lie on the y-axis, and the seam should intersect the negative z-axis if the sphere is located at the origin). See section 4.4.1 of the book.
  <li><strong><tt>2.Triangle</strong></tt>: In our system, each triangle belongs to a <tt>Mesh</tt>, which keeps a reference to a <tt>mesh</tt> object. A triangle stores an <tt>OBJFace</tt> object which corresponds to the face formed by the triangle. See section 4.4.2 of the book.

If the owning face does not have a per-vertex normal, the <tt>intersect</tt>
method should return the triangle normal stored in <tt>Triangle</tt>. If it does,
it should interpolate the normals of the three vertices using the barycentric
coordinates of the hit point. Texture coordinates, if available, should be interpolated using barycentric coordinates as well.
</ul>

<p> To test your implementation, you can run <tt>SphereTests.java<tt> and <tt>TriangleTests.java</tt> in package <tt>ray1.tests</tt>

<h3>Task 3: Scene (10 points)</h3>

<p>The <tt>Scene</tt> class stores information relating to the composition, 
i.e. the camera, a list of surfaces, a list of lights, etc. It also contains a scene-file-specified <tt>exposure</tt> field, which can be used to adjust the brightness of the resulting image. You need to implement the following method: 
<p><code>
  boolean intersect(IntersectionRecord outRecord, Ray rayIn, boolean anyIntersection)
</code>

<p>  The method should loop through all surfaces in the scene (stored in the <tt>Surfaces</tt> field), looking for intersections along <tt>rayIn</tt>.
  This is done by calling the <tt>intersect</tt> method on each surface and it is used when a ray is first cast out into the scene. If <tt>anyIntersection</tt> is false, 
  it should only record the first intersection that happens along <tt>rayIn</tt>, i.e. the intersection   
  with the lowest value of <tt>t</tt>. If there is such an intersection, it is recorded in <tt>outRecord</tt>
  and true is returned; otherwise, the method returns false, and <tt>outRecord</tt> is not modified. 
  If <tt>anyIntersection</tt> is true, the method may return true immediately upon finding any intersection 
  with the scene's geometry, even without setting <tt>outRecord</tt> 
  (this version is used to make shadow calculations faster).

  <p> To test your implementation, you can run <tt>Scene.java<tt> in package <tt>ray1.tests</tt>

<h3>Task 4: Ray Tracing (10 points)</h3>

<p>You then need to complete the <code>shadeRay</code> method of the <code>RayTracer</code> class. 
  This method contains the main logic of the ray tracer. It takes a ray generated by the camera, intersects it with the scene, 
  and returns a color based on what the ray hits. Fortunately, most of the details are implemented in other methods; 
  all you need to do is call those methods, and this should only take a few lines. For instance, the <code>Scene</code> class 
  contains a method for finding the first object a given ray hits. Details are in the code's comments.


<p>We have provided a normal shader that determines color based on the normal of the surface, 
  as well as some sample scenes that use this shader. Once you have completed the previous sections, 
  you should be able to render the scenes <tt>one-sphere-rgb-normals.xml</tt> and <tt>two-boxes-rgb-normals.xml</tt> correctly. 
  See Appendix A for details on rendering scenes.


<h3>Task 5: Shading (30 points)</h3>

<p> Here in this task, you are computing the value returned from a ray. We ask you to implement the Lambertian and Phong shading models, with mirror reflections. 
  You can find the method to be filled in <code>PhongBRDF.java</code>, <code>LambertianBRDF.java</code> and <code>ReflectionShader.java</code></p>

<p>A shader is represented by an instance of the abstract <code>Shader</code> class.  
  The key method in the <code>Shader</code> hierarchy is
<p><code>
  public abstract void shade(Colord outRadiance, Scene scene, Ray ray, IntersectionRecord record, int depth)
</code> 
<p>  which sets <tt>outRadiance</tt> (the resulting color) to the fraction of 
the light that comes from the incoming direction \(\omega_i\) (i.e. the direction towards the light) and scatters out in direction \(\omega_o\) (i.e., towards the viewer).</p>

<p>The framework comes with a shader called <tt>RGBNormals</tt>, which is helpful for debugging your intersection code, but all the "interesting" shaders are subclasses of <tt>ReflectionShader</tt>.  To avoid repetitive code, the <tt>shade</tt> method is implemented here, with the differences between different shading models encapsulated in the <tt>BRDF</tt> class.  You can see that the <tt>Lambertian</tt>, <tt>Phong</tt>, and two Microfacet classes are already implemented, but all they do is construct instances of corresponding BRDF subclasses <tt>LambertianBRDF</tt>, <tt>PhongBRDF</tt>, etc.
</p>

<p>The idea is to compute shading using a common formula that works for all the models:
  \[
  L_o(\omega_o) = L_a = f_r(\mathbf{x}, \omega_i, \omega_o) 
        \max(\mathbf{n} \cdot \omega_i, 0) \frac{I}{r^2}
  \]

where \(\omega_i\) is a unit vector towards the light,  \(L_a\) is the ambient light reflection,  \(\omega_o\) is a unit vector toward the viewer, \(\mathbf{x}\) is a location on the surface, and \(\mathbf{n}\) is the surface normal at \(\mathbf{x}\). \(I\) and \(r^2\) are the intensity and distance of the light source, respectively.  The function \(f_r\) is the BRDF of the surface, and it is different for each material.  The BRDF is represented by the <tt>BRDF</tt> class, with its key method
<p>
<code>
  public abstract void evalBRDF(
      Vector3d incoming, Vector3d outgoing, Vector3d surfaceNormal, 
      Vector2 texCoords, Colorf BRDFValue);
</code>
</p>

<p>We ask you to implement the Lambertian and Phong shading models by implementing  and the evaluation methods <code>LambertianBRDF.evalBRDF</code> and <code>PhongBRDF.evalBRDF</code> (with the Microfacet BRDFs as extra credit: implementing them will automatically enable microfacet shading).

<ul style="list-style: none;">
  <li><tt><strong>ReflectionShader</tt></strong>: In the <code>shade</code> method you will basically evaluate the expression above for \(L_o\) (which corresponds to the output parameter <tt>outRadiance</tt>), using the provided parameters.</p>

  <p>Notice that these values should be computed once for each light in the scene, and the contributions from each light should be summed, unless something is blocking the path from the light source to the object.  Check whether your shading point is shadowed by constructing a ray segment (meaning a <tt>Ray</tt> that has both a starting and ending \(t\) value) that starts at your shading point and ends at the light source, then calling <tt>scene.getAnyIntersection</tt> to check whether it intersects any surfaces.  Don't forget to offset the start of the ray; <tt>Ray.makeOffsetSegment</tt> is the ideal tool for making shadow rays.

<pre>
 <strong>reset</strong> <tt>outRadiance</tt> to (0, 0, 0)
   <strong>for</strong> each light in the scene
     <strong>if</strong> light is not shadowed
       compute color contribution from this light
       add this contribution to <tt>outRadiance</tt>
     <strong>end if</strong> 
   <strong>end for</strong> 
</pre>
</ul>

<ul style="list-style: none;">
  <li><tt><strong>LambertianBRDF</tt></strong>: This class implements the perfectly diffuse BRDF. Its BRDF value is constant, corresponding to the reflectance \(R_d\) specified by the <tt>diffuseColor</tt> field:
  \[
  f_r(\mathbf{x}, \omega_i, \omega_o) = R_d(\mathbf{x}) / \pi
  \]
  See below under Textures for the dependence of \(R_d\) on \(\mathbf{x}\).
Section 4.5.1 of the book is relevant, but note that there are several differences and the course slides are more consistent with this assignment.</li>

<p>After <tt>ReflectionShader</tt> and <tt>LambertianBRDF</tt> are implemented, you should be able to render <tt>two-boxes.xml</tt> correctly.

<li><tt><strong>PhongBRDF</tt></strong>: This class implements the Modified Blinn-Phong lighting model. It has a diffuse  component identical to the Lambertian model, as well as a specular component controlled by the <tt>specularColor</tt>, \(k_s\), and <tt>exponent</tt>, \(p\), fields, where the <tt>exponent</tt> determines the "shininess". The <tt>evalBRDF</tt> method should evaluate this model:
  \[
  f_r(\mathbf{x}, \omega_i, \omega_o) = 
    R_d(\mathbf{x}) / \pi +
    k_s \, \max(\mathbf{n} \cdot \mathbf{h}, 0)^p
  \]
Here, \(\mathbf{h} = \frac{\omega_i \,+\, \omega_o}{|\omega_i \,+\, \omega_o|}\). See section
4.5.2 with the same caution as above.</li>

<p>To avoid some weird artifacts related to the difference between shading normals and geometric normals, after the BRDF function value \(f_r(\omega_i,\omega_o,n)\) computed, the <tt>evalBRDF</tt> function should set <tt>BRDFValue</tt> to:
  \[ f_r(\omega_i,\omega_o,n) \; \max(n \cdot \omega_i, 0) \]
This way, if you manage to get a view direction that actually points <i>into</i> the surface rather than away from it, you will just return zero.
</p>

<p>You can validate your implementation of PhongBRDF and LambertianBRDF by running <tt>PhongTesets.java</tt> and <tt>LambertianTests.java</tt> in package <tt>ray1.tests</tt> </p>

<!-- <p>After both BRDFs are completed, you should be able to render <tt>one-sphere.xml</tt>, <tt>teapot.xml</tt>, <tt>wire-box.xml</tt>, <tt>wire-box-orthographic.xml</tt> and the bunny scenes correctly. -->


</ul>


<h5><strong>Mirror reflection</strong></h5>

<p>Once you have shading based on illumination from light sources working, 
  there is one more kind of reflection to include: reflections of objects in one another's surfaces.  Doing this accurately and consistently with shading from BRDFs is a topic for later in the semester, but for this assignment we simply add a completely separate mirror 
  reflection component to the shading.  It is computed independently of the shading computed using light sources and BRDFs, and it's controlled by the parameter <tt>mirrorCoefficient</tt>, which is defined in 
    <tt>ReflectionShader</tt> and therefore available for all shaders that are subclasses. 
  Then with this mirror reflection, the outgoing refectance turns out to be the sum of reflected ambient light  /( L_a/) and mirror reflected light /( L_r /).
</p>

\[
L_o(\omega_o) = L_a + L_r = L_a + R(\theta) L_o(\omega_r)
\]
  
<p>Where \(L_o\) is the final intensity we compute in <code>ReflectionShade.shade</code>, \(L_r\) is the mirror reflectance we want compute in this task, \(\omega_r\) is the mirror reflection direction.  \(\theta\) is the angle between outgoing direction and the surface normal at hit point \(\mathbf{x}\).

<p>Mirror reflection is computed by recursively calling <code>RayTracer.shadeRay</code> 
  to compute the direction that is seen by a ray starting at the surface and proceeding 
  in the direction that is the mirror reflection of the view direction. 
   Compute the reflection direction as discussed in the book (Section 4.8) 
   and in the slides, find the radiance seen in that direction, scale it by the mirror reflectance and add it into <tt>outRadiance</tt>. 
  The mirror reflectance is computed using Schlick's approximation:
  \[
  R(\theta) = R_0 + (1 - R_0)( 1 - \cos \theta)^5
  \]

   \(R_0\) is the <code>mirrorCoefficient</code> we passed in in xml file.
  <p>You will need to introduce a limit on recursion depth; use the  <tt>depth</tt> argument as a counter to limit recursion to 12 levels deep. 
</p> 


<p>Mirror reflection is not enabled for most scenes because it slows things down a bit; 
  to keep from wasting time when it is turned off, check whether <code>mirrorCoefficient</code> 
  is nonzero before tracing the recursive ray.  Once mirror reflection is working you will 
  be able to render the <tt>four-spheres</tt> scene and see lots of interreflections, 
  and will see the reflections added to the <tt>teapot</tt> scenes.


<h3>Testing Your Code</h3>
<!--(this is some advice we should get in here...)
You can run the program from the command line
fast_scenes are for quick regression testing
don't be afraid to edit the scenes!  Especially to make low res versions for quicker iteration.-->

<p>The <tt>data</tt> directory contains scene files and reference images for the purpose of testing your code. Right click on the 
<tt> RayTracer</tt> file in <tt>Package Explorer</tt>, and choose <tt>Run As</tt> → <tt>Java Application</tt>.  These scene files are organized into several subdirectories:
<ul>
  <li><tt>fast_scenes</tt>: a collection of simple scenes that should render very quickly, under a second each, so that you can quickly tell what is working.  You should re-render these constantly as you work, so that you learn right away if you have broken something that used to work.</li>
  <li><tt>mesh_scenes</tt>: scenes that test the ability to handle larger triangle meshes.  Since your ray tracer takes time linear in the number of triangles, these will be slower to render.  When you are iterating on one of these results, don't hesitate to edit the input file to make the output image temporarily much smaller!</li>
  <li><tt>textured_scenes</tt>: scenes that test the texture mapping features.</li>
</ul>
Without just a directory name passed as a command line argument, the ray tracer will attempt to render all scenes in the corresponding scene directory (i.e. <tt>data/scenes/ray1/<i>dirname</i></tt>).
If you want to render specific scenes, you can list the file names individually on the command line. </p>

<p>You can set up command line arguments from the <tt>Run</tt> dialog (<tt>Run Configurations</tt>) in Eclipse. Choose the <tt>Arguments</tt> tab, and provide your command line arguments in <tt>Program arguments</tt>.  Alternatively, you can run the program from the command line, which is sometimes more convenient for rapid iteration.  You will need to set both the class path and the library path; a command line like the following, executed in the <tt>Ray/</tt> project directory, will do it:
<pre>java -cp bin/:deps/lib/openexrjni-3.0.0.jar -Djava.library.path=deps/native/macosx/ ray1.RayTracer fast_scenes/</pre>  
with an appropriate substitution to the library path for Windows or Linux.

<p>Note that <tt>RayTracer</tt> prepends 
<tt>data/scenes/ray1</tt> to its arguments. This means that you only need to provide a scene file name to render it. For example, if you give the argument <tt>fast_scenes/one-sphere.xml</tt> to the <tt>RayTracer</tt> it will load the scene file <tt>data/scenes/ray1/fast_scenes/one-sphere.xml</tt> and produce an output image in the same directory as the scene file.

<p>If you would like to change the default scene location, there is a <tt>-p</tt> command line option that allows you to specify a different default path instead of using <tt>data/scenes/ray1</tt>. 

<p>Compare the images you have with the reference images; if there are visually
significant differences, consider checking for bugs in your code.


<h4>Appendix A: Test Scenes</h4>
Here we attached a note of the scenes that you can test for different materials:

<p><strong>Lambertian</strong>: two-boxes, one-sphere-lambertian, teapot-lambertian</p>

<p><strong>Phong</strong>: teapot_phong, bunny_phong, wire-box-phong</p>

<p><strong>Microfacet</strong>: one-sphere, one-sphere-GGX, teapot, earth, earth-eclipse-textured, earth-GGX</p>

<p><strong>Lambertian + Microfacet</strong>: bunny.xml, bunny2.xml, bunny_norms.xml, wire-box.xml, wire-box-orthographic, teapot-GGX</p>

<p><strong>Mirror reflection</strong>: four-sphere, teapot_phong_mirror</p>



<h4>Appendix B: Extra Credit</h4>
<h5><strong>Cylinder surface</strong></h5>
<ul>
  <p>We've included a <tt>Cylinder</tt> subclass of <tt>Surface</tt>, similar to the <tt>Triangle</tt> and <tt>Sphere</tt> classes. The class contains <tt>center</tt>, a 3D point, and <tt>radius</tt>, <tt>height</tt>, both real numbers. Assume that the cylinder is capped, i.e., the top and bottom have caps (it is not a hollow tube). Also, assume it is aligned with the z-axis, and is centered around <tt>center</tt>; <br>
  i.e., if
  <tt>center</tt> is \((c_x, c_y, c_z)\) and <tt>height</tt> is \(h\), the cylinder's z-extent is from \(c_z-\frac{h}{2}\) to \(c_z+\frac{h}{2}\).

  <p>Your task is to implement the <tt>intersect</tt> method of a mathematical cylinder (NOT a meshed cylinder). Add an XML scene that includes a cylinder to test your implementation (See Appendix C for details on how this should be done--you shouldn't need to change any parser code).

  <p>Once this is done, try implementing a cylinder that is arbitrarily oriented. You will need to add two parameters, <tt>rotX</tt> and <tt>rotY</tt>, to specify the cylinder's rotation about the x and y axes. Note that this will require changing the coordinate system of the incoming ray within the <tt> intersect</tt> method to account for these rotations. After the coordinate system change, the math used to intersect the ray with the cylinder should be the same as its axis-aligned variant. Finally, build a test scene with a cylinder object.
</ul>

<h5><Strong>Microfacet BRDF</Strong></h5>
<ul>
<li><strong><tt>MicrofacetBRDF</tt></strong>: This and its two subclasses implement two variants of the Microfacet BRDF model. Its BRDF function is product of several factors:
  <ul style="list-style: none; text-align: center;">
    \[ f_r(\omega_i,\omega_o,n) \; =  \; \frac{F(\omega_i,h)G(\omega_i,\omega_o,h)D(h)}{4|\omega_i \cdot n| |\omega_o \cdot n|} \]
  </ul>

  <p><tt>\(\omega_i\)</tt>: Direction unit vector along incoming ray.</p>
  <p><tt>\(\omega_o\)</tt>: Direction unit vector along outgoing ray.</p>
  <p><tt>\(n\)</tt>: unit normal vector of surface at the shaded point.</p>
  <p><tt>\(h\)</tt>: Half vector of \(\omega_i\) and \(\omega_o\) and its length is normalized to unit. 
  <ul style="list-style: none; text-align: center;">
    \[  h(\omega_i, \omega_o) \; = \; \frac{\omega_i + \omega_o}{|\omega_i + \omega_o|} \]
  </ul>
  </p>

  <p>To avoid some weird artifacts related to the difference between shading normals and geometric normals, after the BRDF function value \(f_r(\omega_i,\omega_o,n)\) computed, the <tt>evalBRDF</tt> function should set <tt>BRDFValue</tt> to:
    \[ f_r(\omega_i,\omega_o,n) \; \max(n \cdot \omega_i, 0) \]
  </p>

  <p>More details on each factor of the microfacet BRDF function follow.</p>
  
  <p><tt>\(F(\omega_i,h)\)</tt>: This is the Fresnel term of unpolerized light and only reflection is considered in this assignment.
  <ul style="list-style: none; text-align: center;">
    \[ F(\omega_i, h) \; = \; \frac{1}{2}\frac{(g-c)^2}{(g+c)^2}\left(1 \; + \; \frac{(c(g+c)-1)^2}{(c(g-c)+1)^2}\right)\]
  </ul>
  where \(g \; = \; \sqrt{\frac{n_t^2}{n_i^2}-1+c^2}\) and \(c \; = \; |\omega_i \cdot h|\). \(n_t\) and \(n_i\) are the refractive index of material and air respectively. \(n_i\;=\;1.0\). In microfacet shader, there is a variable <tt>nt</tt> parsed from xml file and it corresponds to \(n_t\). Note that the value of \(g\) can be imaginary for some materials, clamp \(F\;=\;1\) in this case.
</p>

<p>The \(G\) and \(D\) functions differ for the two variants.</p>

<li><strong><tt>BeckmannBRDF</tt></strong>: this class implements the Beckmann microfacet distribution.

  <p><tt>\(D(h)\)</tt>: This factor comes from the microfacet distribution function. For the Beckmann model it is:
    \[ D(h) \; = \; \frac{\chi^{+}(h \cdot n)}{\pi\alpha_b^2\cos^2\theta_h} \; e^{\frac{-\tan^2\theta_h}{\alpha_b^2}} \]
  where \(\theta_h\) denotes the angle between \(h\) and \(n\); \(\chi^{+}(a)\) is positive characteristic function (one if \(a>0\) and zero if \(a \leq 0\) ). \(\alpha_b\) is width parameter of the Beckmann distribution, corresponding to parsed variable <tt>alpha</tt> in microfacet shader.</p>

<p>
  <tt>\(G(\omega_i,\omega_o,h)\)</tt>: This is Smith shadowing-masking term and depends on the choice of microfacet distribution function \(D(h)\). \(G(\omega_i, \omega_o,h)\) can be separated as the product of two parts:
    \[ G(\omega_i, \omega_o, h) = G_1(\omega_i,h)G_1(\omega_o,h) \]
  For Beckmann distribution,
    \[ 
    G_1(v,h) \; = \; \chi^{+}\left(\frac{v\cdot h}{v\cdot n}\right)
    \begin{cases}
    \frac{3.535a\;+\;2.181a^2}{1\;+\;2.276a\;+\;2.577a^2}, & \text{if}\ a < 1.6 \\
    1 & \text{otherwise}
    \end{cases}
     \]
  where \(a=(\alpha_b\tan\theta_v)^{-1}\) and \(\theta_v\) is the angle between \(v\) and \(n\).
</p>

<li><strong><tt>GGXBRDF</tt></strong>: The microfacet-based shader implemented above assumes the normal distribution of the microfacet surface is Beckmann. Another class of normal distribution that is widely used is GGX. In order to implement microfacet shader with GGX distribution, both the Smith shadowing-masking factor \(G(\omega_i,\omega_o,h)\) and the microfacet distribution factor \(D(h)\) should be modified. </p>
  <p>
  The GGX distribution with width parameter \(\alpha_gs\) is 
    \[ D(h) \; = \; \frac{\alpha_g^2\;\chi^{+}(h \cdot n)}{\pi\cos^4\theta_h\;(\alpha_g^2\;+\;\tan^2\theta_h)^2} \]
  The corresponding shadowing-masking term is
    \[ G(\omega_i, \omega_o, h) = G_1(\omega_i,h)G_1(\omega_o,h) \]
  And for GGX distribution,
    \[ 
    G_1(v,h) \; = \; \chi^{+}\left(\frac{v\cdot h}{v\cdot n}\right)
    \; \frac{2}{1\;+\;\sqrt{1\;+\;\alpha_g^2\tan^2\theta_v}}
     \]
  The meaning of the variable notations and all other factors in the BRDF function keep the same as above.</p>
</ul>


  <tt><strong>Textures</strong></tt>

<p>The <tt>Shader</tt> classes above have a <tt>texture</tt> field that is by default set to <tt>null</tt>. If the scene XML file specifies an image texture for the shader, a <tt>Texture</tt> object will be referenced by this field.  In this case, you should use the texture to determine the diffuse color in the <tt>shade</tt> method, ignoring the <tt>diffuseColor</tt> field.</p>

<p>You will see that all the <tt>ReflectionShader</tt> subclasses set up the <tt>BRDF</tt>s with a reference to the texture also.  Thus, whenever you are computing diffuse shading, you should get the reflectance from the texture if available, and otherwise the diffuse color.  This logic is encapsulated in the helper method <tt>BRDF.getDiffuseReflectance</tt> which is available to all the BRDF methods you're implementing.

<p>The <tt>Texture</tt> class and its subclasses are responsible for using the UV-coordinates of a particular point on the <tt>Surface</tt> to find this color on a given image (located in the <tt>Texture</tt> class). You will be implementing the <tt>getTexColor(Vector2d texCoord)</tt> method for two very similar subclasses of
 <tt>Texture</tt>. Both classes convert UV-coordinates into the nearest pixel coordinates corresponding to the image. The difference is in the way they handle UV-coordinates outside of the [0.0, 1.0] range:

<ul style="list-style: none;">
  <li><strong><tt>ClampTexture</tt></strong>: This class uses the nearest pixel for UV-coordinates that are out of range. As a result, the colors of the boundary pixels of a texture are extended outwards in UV space.
  <li><strong><tt>RepeatTexture</tt></strong>: This class repeats the texture for out-of-range UV-coordinates. In this case, copies of the texture are tiled across UV space.
</ul>

<p>The details of how to implement these are provided in the code.  Once these are completed, all scenes in subdirectory <tt>textured_scenes</tt> should render correctly.</li>


<h4>Appendix C: Framework</h4>

<p>This section describes the framework code that comes with the assignment. You do not need to spend time trying to understand it and it is not essential to read this to get started on the assignment. Instead, you can reference it as needed.

<p>The framework for this assignment includes a simple main program,
some utility classes for vector math, a parser for the input file
format, and stubs for the classes that are required by the parser.

<h5><strong>RayTracer</strong></h5>

<p>This class holds the entry point for the program. The <tt>main</tt>
method is provided, so that your program will have a command-line
interface compatible with ours. It treats each command line argument
as the name of an input file, which it parses, renders an image, and
writes the image to an EXR file. The method <tt>RayTracer.renderImage</tt> is called to do the actual rendering.

<h5><strong>Image</strong></h5>

<p>This class contains an array of pixel colors (stored as doubles) and the requisite
code to get and set pixels and to output the image to a file.

<h5><strong><tt>egl.math</tt> package</strong></h5>

<p>This package contains classes to represent 2D and 3D vectors,
as well as RGB colors. They support all the standard vector arithmetic
operations you're likely to need, including dot and cross products
for vectors and addition and scaling for colors.

<h5><strong>Parser</strong></h5>

<p>The <tt>Parser</tt> class contains a simple parser based on Java's built-in XML
parsing. The parser simply reads a XML document and instantiates an object for
each XML entity, adding it to its containing element by calling
<tt>set...</tt> or <tt>add...</tt> methods on the containing object.

<p>For instance, the input:
<pre>
&lt;scene&gt;
    &lt;surface type="Sphere"&gt;
        &lt;shader type="Lambertian"&gt;
            &lt;diffuseColor&gt;0 0 1&lt;/diffuseColor&gt;
        &lt;/shader&gt;
        &lt;center&gt;1 2 3&lt;/center&gt;
        &lt;radius&gt;4&lt;/radius&gt;
    &lt;/surface&gt;
&lt;/scene&gt;
</pre>
results in the following construction sequence:
<ul>
  <li>Create the scene.
  <li>Create an object of class <tt>Sphere</tt> and add it to the scene
by calling <tt>Scene.addSurface</tt>. This is OK because <tt>Sphere</tt>
extends the <tt>Surface</tt> class.
  <li>Create an object of class <tt>Lambertian</tt> and add it to the sphere using <tt>Sphere.setShader</tt>. 
This is OK because <tt>Lambertian</tt> extends the <tt>Shader</tt> class.
 <li>Call <tt>setDiffuseColor(new Colord(0, 0, 1))</tt> on the shader.
 <li>Call <tt>setCenter(new Vector3d(1, 2, 3))</tt> on the sphere.
 <li>Call <tt>setRadius(4)</tt> on the sphere.
</ul>

<p>Which elements are allowed where in the file is determined by which
classes contain appropriate methods, and the types of those methods'
parameters determine how the tag's contents are parsed (as a number,
a vector, etc.). There is more detail for the curious in the header
comment of the <tt>Parser</tt> class.

<p>The practical result of all this is that your ray tracer is handed
an object of class <tt>Scene</tt> that contains objects that are in one-to-one correspondence
with the elements in the input file. You shouldn't need to change
the parser in any way.

<h4>What to Submit</h4>
<p>Submit a ZIP file containing your solution organized the same way as the code on Github, along with any additional files.
Include a readme in your zip that contains:
<ul>
  <li>You and your partner's names and NetIDs.
  <li>Any problems with your solution.
  <li>Anything else you want us to know.
</ul>
<p><strong><a href=https://cms.csuglab.cornell.edu>Upload here (CMS)</a></strong>
